# PropFirms Data Aggregation - Detailed Technical Analysis

**Date**: 2026-02-13
**Scope**: Deep dive into data synchronization, API limits, storage trade-offs, and test coverage strategy

---

## Table of Contents

1. [Arbiscan API Integration](#1-arbiscan-api-integration)
2. [Real-time Sync (24h Supabase)](#2-real-time-sync-24h-supabase)
3. [Historical Sync (Monthly JSON)](#3-historical-sync-monthly-json)
4. [Data Overlap & Accuracy](#4-data-overlap--accuracy)
5. [File I/O Performance Analysis](#5-file-io-performance-analysis)
6. [Storage Cost Comparison](#6-storage-cost-comparison)
7. [Test Coverage Strategy](#7-test-coverage-strategy)
8. [Failure Modes & Mitigations](#8-failure-modes--mitigations)

---

## 1. Arbiscan API Integration

### 1.1 API Specification

**Base URL**: `https://api.etherscan.io/v2/api`
**Chain ID**: `42161` (Arbitrum One)
**Documentation**: https://docs.etherscan.io/etherscan-v2

**Endpoints Used**:
```javascript
// Native ETH transactions
GET /v2/api?chainid=42161&module=account&action=txlist&address={addr}&apikey={key}

// ERC-20 token transactions
GET /v2/api?chainid=42161&module=account&action=tokentx&address={addr}&apikey={key}
```

**Response Format**:
```json
{
  "status": "1",
  "message": "OK",
  "result": [
    {
      "hash": "0xabc...",
      "from": "0x123...",
      "to": "0x456...",
      "value": "1000000000000000000",
      "timeStamp": "1707234567",
      "tokenSymbol": "USDC",
      "tokenDecimal": "6"
    }
  ]
}
```

### 1.2 Rate Limits (Free Tier)

**Official Limits**:
- **5 calls per second**
- **100,000 calls per day**
- **1,000 records per call** (pagination required for more)

**Error Responses**:
```json
// Rate limit exceeded
{
  "status": "0",
  "message": "Max rate limit reached",
  "result": []
}

// Invalid API key
{
  "status": "0",
  "message": "Invalid API Key",
  "result": "Error!"
}

// No data
{
  "status": "0",
  "message": "No transactions found",
  "result": []
}
```

**Current Error Handling**:
```javascript
// lib/arbiscan.js:32-54
if (data.status === '0') {
  // No transactions found - return empty array ‚úÖ
  if (data.message === 'No transactions found') {
    return [];
  }

  // Rate limiting - log and return empty ‚ö†Ô∏è
  if (data.message.includes('rate limit')) {
    console.warn(`[Arbiscan] Rate limit hit for address ${address}`);
    return []; // Silent failure - no retry!
  }

  // Invalid API key - return empty ‚ùå
  if (data.message.includes('Invalid API Key')) {
    console.error(`[Arbiscan] API key issue: ${data.message}`);
    return []; // Should throw error!
  }
}
```

**üî¥ CRITICAL ISSUES**:
1. ‚ùå **Silent failures** on rate limits - returns empty array, sync appears successful
2. ‚ùå **No retry logic** - transient errors are not retried
3. ‚ùå **No exponential backoff** - violates API best practices
4. ‚ùå **No circuit breaker** - continues hammering failed API

### 1.3 Current Usage Patterns

**Sync Frequency**: Every 5 minutes via Inngest cron

**Per-Firm API Calls**:
```javascript
// For a firm with N addresses
const callsPerFirm = N * 2; // (native + token) √ó addresses

// Example: fundingpips (3 addresses)
// ‚Üí 3 √ó 2 = 6 API calls per sync
```

**Rate Limit Compliance**:
```javascript
// lib/services/payoutSyncService.js:149-152
// Between addresses (within same firm)
await new Promise(r => setTimeout(r, 500)); // 500ms = 2 calls/sec ‚úÖ

// Between firms
await new Promise(r => setTimeout(r, 1000)); // 1000ms = 1 call/sec ‚úÖ
```

**Daily Usage Calculation** (current: 10 firms, avg 2.5 addresses):
```
API calls per sync:
  10 firms √ó 2.5 addresses √ó 2 endpoints = 50 calls

Syncs per day:
  12 syncs/hour √ó 24 hours = 288 syncs

Total daily calls:
  288 √ó 50 = 14,400 calls/day

Percentage of limit:
  14,400 / 100,000 = 14.4% ‚úÖ
```

### 1.4 Scaling Analysis

**Maximum Firms on Free Tier**:
```javascript
// Given:
// - Daily limit: 100,000 calls
// - Sync frequency: every 5 min (288 syncs/day)
// - Avg addresses per firm: 2.5
// - Calls per address: 2

const maxCalls = 100_000;
const syncsPerDay = 288;
const addressesPerFirm = 2.5;
const callsPerAddress = 2;

const maxFirms = Math.floor(
  maxCalls / (syncsPerDay * addressesPerFirm * callsPerAddress)
);
// = 100,000 / (288 √ó 2.5 √ó 2)
// = 100,000 / 1,440
// = 69.4 firms
```

**üü† WARNING**: Current architecture **caps at ~70 firms** on free tier.

**Scaling Strategies**:

| Strategy | Max Firms | Cost | Complexity |
|----------|-----------|------|------------|
| **Current (5min sync)** | 70 | $0 | Low |
| **10min sync** | 140 | $0 | Low |
| **15min sync** | 210 | $0 | Low |
| **Tiered (hot/cold)** | 150+ | $0 | Medium |
| **Paid tier (200k/day)** | 140 | $49/mo | Low |
| **Pro tier (500k/day)** | 350 | $149/mo | Low |

**Recommended Tiered Approach**:
```javascript
// Prioritize firms by activity
const hotFirms = firms.filter(f => f.last_payout_at > Date.now() - 24*60*60*1000);
const coldFirms = firms.filter(f => f.last_payout_at <= Date.now() - 24*60*60*1000);

// Sync hot firms every 5 min, cold firms every 30 min
if (isHotSyncTime()) {
  await syncFirms(hotFirms);
}
if (isColdSyncTime()) {
  await syncFirms(coldFirms);
}
```

### 1.5 Recommended Improvements

**Priority 1: Error Handling**
```javascript
// lib/arbiscan.js (enhanced)
async function fetchWithRetry(url, retries = 3, backoff = 1000) {
  for (let attempt = 1; attempt <= retries; attempt++) {
    try {
      const response = await fetch(url);
      const data = await response.json();

      if (data.status === '1') {
        return data.result || [];
      }

      // Rate limit - exponential backoff
      if (data.message.includes('rate limit')) {
        const delay = backoff * Math.pow(2, attempt - 1);
        console.warn(`[Arbiscan] Rate limit, retry ${attempt}/${retries} after ${delay}ms`);

        if (attempt < retries) {
          await sleep(delay);
          continue;
        }

        // Final retry failed - throw
        throw new Error(`Rate limit exceeded after ${retries} retries`);
      }

      // Invalid API key - don't retry
      if (data.message.includes('Invalid API Key')) {
        throw new Error('Invalid Arbiscan API key');
      }

      // No data - not an error
      if (data.message === 'No transactions found') {
        return [];
      }

      // Unknown error
      throw new Error(`Arbiscan error: ${data.message}`);

    } catch (error) {
      if (attempt === retries) throw error;
      await sleep(backoff * Math.pow(2, attempt - 1));
    }
  }
}
```

**Priority 2: Circuit Breaker**
```javascript
// lib/arbiscan.js
class ArbiscanCircuitBreaker {
  constructor(threshold = 5, timeout = 60000) {
    this.failureCount = 0;
    this.threshold = threshold;
    this.timeout = timeout;
    this.state = 'CLOSED'; // CLOSED, OPEN, HALF_OPEN
    this.nextAttempt = null;
  }

  async execute(fn) {
    if (this.state === 'OPEN') {
      if (Date.now() < this.nextAttempt) {
        throw new Error('Circuit breaker is OPEN');
      }
      this.state = 'HALF_OPEN';
    }

    try {
      const result = await fn();
      this.onSuccess();
      return result;
    } catch (error) {
      this.onFailure();
      throw error;
    }
  }

  onSuccess() {
    this.failureCount = 0;
    this.state = 'CLOSED';
  }

  onFailure() {
    this.failureCount++;
    if (this.failureCount >= this.threshold) {
      this.state = 'OPEN';
      this.nextAttempt = Date.now() + this.timeout;
      console.error(`[Circuit Breaker] OPEN - too many failures (${this.failureCount})`);
    }
  }
}

const circuitBreaker = new ArbiscanCircuitBreaker();

export async function fetchNativeTransactions(address, apiKey) {
  return circuitBreaker.execute(() => fetchWithRetry(...));
}
```

**Priority 3: Usage Monitoring**
```javascript
// lib/arbiscan.js
class ArbiscanUsageTracker {
  constructor() {
    this.dailyCalls = 0;
    this.lastReset = Date.now();
  }

  trackCall() {
    // Reset counter at midnight UTC
    const now = Date.now();
    if (now - this.lastReset > 24 * 60 * 60 * 1000) {
      this.dailyCalls = 0;
      this.lastReset = now;
    }

    this.dailyCalls++;

    // Alert at 80% usage
    if (this.dailyCalls >= 80000 && this.dailyCalls % 1000 === 0) {
      console.warn(`[Arbiscan] High API usage: ${this.dailyCalls}/100000`);
    }

    return {
      calls: this.dailyCalls,
      limit: 100000,
      percentage: (this.dailyCalls / 100000) * 100,
    };
  }
}
```

---

## 2. Real-time Sync (24h Supabase)

### 2.1 Architecture

**Trigger**: Inngest cron (`*/5 * * * *` = every 5 minutes)

**Entry Point**:
```javascript
// libs/inngest-payouts.ts:5-21
export const syncPropFirmPayouts = inngest.createFunction(
  { id: "sync-prop-firm-payouts" },
  { cron: "*/5 * * * *" },
  async ({ step }) => {
    return await step.run("sync-all-firms", async () => {
      return await syncAllFirms();
    });
  }
);
```

**Execution Flow**:
```javascript
// lib/services/payoutSyncService.js:275-324
async function syncAllFirms() {
  const supabase = createServiceClient(); // Service role key

  // 1. Fetch all firms
  const { data: firms } = await supabase
    .from('firms')
    .select('id, name, addresses');

  // 2. Sync each firm sequentially
  for (const firm of firms) {
    await syncFirmPayouts(firm);
    await sleep(1000); // Rate limiting
  }

  // 3. Cleanup old payouts
  await cleanupOldPayouts(24);

  return summary;
}
```

### 2.2 Data Processing Pipeline

**Step 1: Fetch Arbiscan Data**
```javascript
// lib/services/payoutSyncService.js:115-153
for (const address of firm.addresses) {
  const [native, tokens] = await Promise.all([
    fetchNativeTransactions(address, apiKey),
    fetchTokenTransactions(address, apiKey),
  ]);

  allNative = [...allNative, ...native];
  allTokens = [...allTokens, ...tokens];

  await sleep(500); // Rate limit protection
}
```

**Step 2: Filter to 24h Window**
```javascript
// lib/services/payoutSyncService.js:53-107
function processPayouts(nativeData, tokenData, sourceAddresses, firmId) {
  const now = Date.now() / 1000;
  const cutoff24h = now - (24 * 60 * 60); // ‚ö†Ô∏è UTC only!

  // Filter native ETH
  const nativePayouts = nativeData
    .filter(tx => lowerAddresses.includes(tx.from.toLowerCase()))
    .filter(tx => parseInt(tx.timeStamp) >= cutoff24h) // Cutoff applied here
    .map(tx => ({
      tx_hash: tx.hash,
      firm_id: firmId,
      amount: (parseFloat(tx.value) / 1e18) * PRICES.ETH,
      payment_method: 'crypto',
      timestamp: new Date(parseInt(tx.timeStamp) * 1000).toISOString(),
      from_address: tx.from,
      to_address: tx.to,
    }));

  // Similar for tokens...

  // Filter spam (<$10)
  return payouts.filter(p => p.amount >= 10);
}
```

**Step 3: Deduplicate & Upsert**
```javascript
// lib/services/payoutSyncService.js:166-173
const { error: upsertError } = await supabase
  .from('recent_payouts')
  .upsert(payouts, { onConflict: 'tx_hash' }); // ‚úÖ Prevents duplicates

if (upsertError) {
  throw new Error(`Upsert failed: ${upsertError.message}`);
}
```

**Step 4: Update Firm Metadata**
```javascript
// lib/services/payoutSyncService.js:200-239
async function updateFirmLastPayout(firmId, latestPayout) {
  const { data: firm } = await supabase
    .from('firms')
    .select('last_payout_at')
    .eq('id', firmId)
    .single();

  const existingTimestamp = firm?.last_payout_at ? new Date(firm.last_payout_at) : null;
  const newTimestamp = new Date(latestPayout.timestamp);

  // Only update if newer ‚úÖ
  if (!existingTimestamp || newTimestamp > existingTimestamp) {
    await supabase
      .from('firms')
      .update({
        last_payout_at: latestPayout.timestamp,
        last_payout_amount: latestPayout.amount,
        last_synced_at: new Date().toISOString(),
      })
      .eq('id', firmId);
  }
}
```

**Step 5: Cleanup Old Payouts**
```javascript
// lib/services/payoutSyncService.js:248-266
async function cleanupOldPayouts(hoursToKeep = 24) {
  const cutoffDate = new Date(Date.now() - (hoursToKeep * 60 * 60 * 1000)).toISOString();

  const { count } = await supabase
    .from('recent_payouts')
    .delete()
    .lt('timestamp', cutoffDate);

  return { deleted: count || 0 };
}
```

### 2.3 Cutoff Time Analysis

**Current Implementation**:
```javascript
const now = Date.now() / 1000; // Unix timestamp (seconds)
const cutoff24h = now - (24 * 60 * 60); // 86400 seconds ago

// Filter: tx.timeStamp >= cutoff24h
```

**Example Scenario** (sync at `2025-02-13 10:05:00 UTC`):
```
Current time: 1739439900 (2025-02-13 10:05:00 UTC)
Cutoff:       1739353500 (2025-02-12 10:05:00 UTC)

Included: All transactions from 2025-02-12 10:05:00 onwards
Excluded: All transactions before 2025-02-12 10:05:00
```

**üî¥ CRITICAL ISSUE: Rolling Window Gaps**

**Problem**: If a sync fails at `T=10:05`, next sync at `T=10:10` has a different cutoff:
```
Sync 1 (10:05): Cutoff = 10:05 yesterday ‚Üí Includes tx at 10:04 yesterday ‚úÖ
[Sync fails or is skipped]
Sync 2 (10:10): Cutoff = 10:10 yesterday ‚Üí Excludes tx at 10:04-10:09 yesterday ‚ùå
```

**Data Gap**: Transactions between `10:04-10:10 yesterday` are **permanently lost**.

**Mitigation**:
```javascript
// Option 1: Extend window during recovery
async function syncFirmPayouts(firm) {
  const lastSyncedAt = firm.last_synced_at;
  const timeSinceLastSync = Date.now() - new Date(lastSyncedAt).getTime();

  // If last sync was >10 min ago, extend cutoff
  const hoursToFetch = timeSinceLastSync > 10 * 60 * 1000
    ? 25 // Extra hour buffer
    : 24;

  const cutoff = (Date.now() / 1000) - (hoursToFetch * 60 * 60);

  // ... rest of sync logic
}
```

### 2.4 Database Schema

**Table: `recent_payouts`**
```sql
CREATE TABLE recent_payouts (
  tx_hash TEXT PRIMARY KEY,           -- Unique transaction hash
  firm_id TEXT NOT NULL,              -- Foreign key to firms.id
  amount NUMERIC NOT NULL,            -- Amount in USD
  payment_method TEXT NOT NULL,       -- 'rise' | 'crypto' | 'wire'
  timestamp TIMESTAMPTZ NOT NULL,     -- When payout occurred
  from_address TEXT,                  -- Wallet that sent
  to_address TEXT,                    -- Wallet that received
  created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_recent_payouts_firm_timestamp
  ON recent_payouts(firm_id, timestamp DESC);

CREATE INDEX idx_recent_payouts_timestamp
  ON recent_payouts(timestamp)
  WHERE timestamp >= NOW() - INTERVAL '24 hours'; -- ‚ö†Ô∏è Needs recreation daily!
```

**üü† WARNING**: Partial index on `timestamp` requires daily recreation or PostgreSQL auto-vacuum.

**Table: `firms`**
```sql
CREATE TABLE firms (
  id TEXT PRIMARY KEY,
  name TEXT NOT NULL,
  addresses TEXT[] NOT NULL,           -- Array of wallet addresses
  last_payout_at TIMESTAMPTZ,          -- Most recent payout timestamp
  last_payout_amount NUMERIC,          -- Amount of last payout
  last_payout_tx_hash TEXT,            -- Hash of last payout
  last_payout_method TEXT,             -- Method of last payout
  last_synced_at TIMESTAMPTZ,          -- When sync last ran
  updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

### 2.5 Performance Characteristics

**Query Performance** (10 firms, ~500 payouts in 24h):
```sql
-- Used by /api/v2/propfirms?period=1d
SELECT firm_id, amount
FROM recent_payouts
WHERE timestamp >= NOW() - INTERVAL '24 hours'
ORDER BY timestamp DESC;

-- Execution time: ~50-100ms (with index)
-- Without index: ~500ms+
```

**Upsert Performance**:
```sql
-- Batch upsert (50 payouts per firm)
INSERT INTO recent_payouts (tx_hash, firm_id, ...)
VALUES ($1, $2, ...), ($3, $4, ...)
ON CONFLICT (tx_hash) DO UPDATE SET ...;

-- Execution time: ~30ms per firm
```

**Total Sync Time** (10 firms):
```
Arbiscan fetches: ~40 seconds
Processing: ~5 seconds
Database upserts: ~300ms (10 firms √ó 30ms)
Cleanup: ~50ms

Total: ~45 seconds ‚úÖ (under 60s timeout)
```

**Scaling Projection** (100 firms):
```
Arbiscan fetches: ~400 seconds (rate limited)
Processing: ~50 seconds
Database upserts: ~3 seconds
Cleanup: ~100ms

Total: ~453 seconds üî¥ (exceeds 60s timeout!)
```

**üî¥ CRITICAL**: Current sync will **timeout** at ~25 firms on Vercel Pro (60s limit).

**Solution**: Parallel sync with batching:
```javascript
async function syncAllFirms() {
  const firms = await fetchFirms();

  // Process in batches of 5
  for (let i = 0; i < firms.length; i += 5) {
    const batch = firms.slice(i, i + 5);
    await Promise.all(batch.map(syncFirmPayouts));
  }
}
```

---

## 3. Historical Sync (Monthly JSON)

### 3.1 Architecture

**Trigger**: GitHub Actions cron (daily at 3 AM PST)

**Workflow File**: [`.github/workflows/sync-firm-payouts-historical.yml`](.github/workflows/sync-firm-payouts-historical.yml:14)

**Schedule**:
```yaml
schedule:
  # 3 AM PST = 11:00 UTC (PST is UTC-8)
  - cron: "0 11 * * *"
```

**Execution Flow**:
```bash
1. Checkout repo (full git history)
2. Install Node.js 20 + dependencies
3. Run: node scripts/update-firm-monthly-json.js
4. Check for file changes in data/payouts/
5. If changed:
   - Git commit
   - Git push to main
6. Deploy triggered via Vercel (auto-deploy on push)
```

### 3.2 Data Processing Pipeline

**Script**: [`scripts/update-firm-monthly-json.js`](scripts/update-firm-monthly-json.js)

**Step 1: Determine Current Month (Timezone-Aware)**
```javascript
// scripts/update-firm-monthly-json.js:83-92
function getCurrentYearMonthInTimezone(timezone) {
  const now = new Date();
  const formatter = new Intl.DateTimeFormat('en-CA', {
    timeZone: timezone,
    year: 'numeric',
    month: '2-digit',
  });
  return formatter.format(now).slice(0, 7); // YYYY-MM
}

// Example:
// UTC time: 2025-02-01 03:00:00
// Dubai (UTC+4): 2025-02-01 07:00:00 ‚Üí "2025-02"
// LA (UTC-8): 2025-01-31 19:00:00 ‚Üí "2025-01"
```

**üî¥ CRITICAL**: Timezone handling can cause **month boundary issues**.

**Scenario**:
```
GitHub Actions runs: 2025-02-01 11:00 UTC (3 AM PST)

Firm: fundingpips (timezone: "Asia/Dubai", UTC+4)
Current month in Dubai: 2025-02-01 15:00 ‚Üí "2025-02" ‚úÖ

Firm: la-based-firm (timezone: "America/Los_Angeles", UTC-8)
Current month in LA: 2025-02-01 03:00 ‚Üí "2025-02" ‚úÖ

But if a transaction occurred at:
  UTC: 2025-02-01 02:00
  Dubai: 2025-02-01 06:00 ‚Üí Month: "2025-02" ‚úÖ Included
  LA: 2025-01-31 18:00 ‚Üí Month: "2025-01" ‚ùå Wrong file!
```

**Step 2: Fetch ALL Transactions from Arbiscan**
```javascript
// scripts/update-firm-monthly-json.js:127-138
async function fetchAllTransactions(address, apiKey) {
  const nativeUrl = `${ARBISCAN_API_BASE}?...&address=${address}&sort=desc&apikey=${apiKey}`;
  const native = await fetchWithRetry(nativeUrl);

  await sleep(300); // Rate limit

  const tokenUrl = `${ARBISCAN_API_BASE}?...&action=tokentx&address=${address}...`;
  const tokens = await fetchWithRetry(tokenUrl);

  return { native, tokens };
}
```

**‚ö†Ô∏è INEFFICIENCY**: Fetches **all-time data** every day, even though only current month is needed.

**Why?**
- Arbiscan API doesn't support time-range filtering efficiently
- `startblock` and `endblock` params exist but require block number calculation
- Simpler to fetch all and filter client-side

**Cost**:
```
Firm with 10k historical transactions:
  - API response time: ~5-10 seconds
  - Data transfer: ~2-5 MB per address
  - Daily waste: fetching 9,950 irrelevant transactions
```

**Better Approach** (future optimization):
```javascript
// Calculate block range for current month
const monthStart = new Date('2025-02-01').getTime() / 1000;
const avgBlockTime = 0.25; // Arbitrum: ~4 blocks/second
const currentBlock = await getLatestBlock();
const monthStartBlock = currentBlock - ((Date.now()/1000 - monthStart) / avgBlockTime);

// Fetch only recent blocks
const url = `...&startblock=${monthStartBlock}&endblock=99999999`;
```

**Step 3: Filter to Current Month (Timezone-Aware)**
```javascript
// scripts/update-firm-monthly-json.js:155-218
function processTransactionsForMonth(native, tokens, addresses, firmId, yearMonth, timezone) {
  const payouts = [];

  for (const tx of native) {
    const timestamp = parseInt(tx.timeStamp);
    const isoTimestamp = new Date(timestamp * 1000).toISOString();

    // Convert to firm's local date
    const txYearMonth = getLocalYearMonth(isoTimestamp, timezone);

    // Filter to target month
    if (txYearMonth !== yearMonth) continue; // ‚úÖ Timezone-aware filtering

    payouts.push({
      tx_hash: tx.hash,
      amount: (parseFloat(tx.value) / 1e18) * PRICES.ETH,
      timestamp: isoTimestamp,
      // ...
    });
  }

  return payouts;
}
```

**Step 4: Build Daily Buckets (Timezone-Aware)**
```javascript
// scripts/update-firm-monthly-json.js:228-268
function buildMonthData(firmId, yearMonth, transactions, timezone) {
  const dailyMap = {};

  for (const t of transactions) {
    const day = getLocalDate(t.timestamp, timezone); // ‚úÖ Convert to local date

    if (!dailyMap[day]) {
      dailyMap[day] = { date: day, total: 0, rise: 0, crypto: 0, wire: 0 };
    }

    dailyMap[day].total += t.amount;
    dailyMap[day][t.payment_method] += t.amount;
  }

  return {
    firmId,
    period: yearMonth,
    timezone: timezone, // ‚úÖ Stored for reference
    summary: { totalPayouts, payoutCount, largestPayout, avgPayout },
    dailyBuckets: Object.values(dailyMap).sort((a, b) => a.date.localeCompare(b.date)),
    transactions: transactions, // Full list for top payouts feature
  };
}
```

**Step 5: Compare & Save**
```javascript
// scripts/update-firm-monthly-json.js:344-367
async function updateFirmMonth(firm, apiKey) {
  // ... fetch & process ...

  const existing = loadExistingMonthData(firm.id, yearMonth);
  const existingCount = existing?.summary?.payoutCount || 0;

  if (existingCount === monthData.summary.payoutCount) {
    console.log(`No new payouts (still ${existingCount})`);
    return { firm: firm.id, changed: false };
  }

  // Save updated file
  fs.writeFileSync(
    `data/payouts/${firm.id}/${yearMonth}.json`,
    JSON.stringify(monthData, null, 2)
  );

  return {
    firm: firm.id,
    changed: true,
    newPayouts: monthData.summary.payoutCount - existingCount,
  };
}
```

### 3.3 File Structure

**Directory Layout**:
```
data/payouts/
‚îú‚îÄ‚îÄ aquafunded/
‚îÇ   ‚îú‚îÄ‚îÄ 2025-01.json (23 KB)
‚îÇ   ‚îî‚îÄ‚îÄ 2025-02.json (TBD)
‚îú‚îÄ‚îÄ blueguardian/
‚îÇ   ‚îú‚îÄ‚îÄ 2024-12.json
‚îÇ   ‚îú‚îÄ‚îÄ 2025-01.json (57 KB)
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ fundingpips/
‚îÇ   ‚îú‚îÄ‚îÄ 2024-10.json (610 KB) ‚Üê Large file warning!
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ the5ers/
    ‚îú‚îÄ‚îÄ 2025-07.json (46 KB)
    ‚îú‚îÄ‚îÄ 2025-08.json (559 KB)
    ‚îî‚îÄ‚îÄ ...
```

**JSON File Format**:
```json
{
  "firmId": "fundingpips",
  "period": "2025-02",
  "timezone": "Asia/Dubai",
  "generatedAt": "2025-02-13T11:23:45.123Z",
  "summary": {
    "totalPayouts": 1234567,
    "payoutCount": 542,
    "largestPayout": 25000,
    "avgPayout": 2276
  },
  "dailyBuckets": [
    {
      "date": "2025-02-01",
      "total": 45000,
      "rise": 30000,
      "crypto": 15000,
      "wire": 0
    },
    ...
  ],
  "transactions": [
    {
      "tx_hash": "0xabc...",
      "firm_id": "fundingpips",
      "amount": 1500,
      "payment_method": "rise",
      "timestamp": "2025-02-01T14:23:45.000Z",
      "from_address": "0x123...",
      "to_address": "0x456..."
    },
    ...
  ]
}
```

### 3.4 Git Commit Strategy

**Commit Message Format**:
```
chore: update firm payout data 2025-02-13
```

**Commit Contents**:
```bash
# Typically 5-10 files change per day
data/payouts/aquafunded/2025-02.json
data/payouts/blueguardian/2025-02.json
data/payouts/fundingpips/2025-02.json
data/payouts/fxify/2025-02.json
...
```

**Git History Growth**:
```
Daily commit size: ~500 KB (10 files √ó 50 KB avg)
Monthly growth: ~15 MB
Yearly growth: ~180 MB

Mitigation:
- Use Git LFS for JSON files >1MB
- OR: Prune old month files (keep last 13 months only)
- OR: Move to database storage
```

---

## 4. Data Overlap & Accuracy

### 4.1 The Overlap Problem

**Real-time (Supabase) Cutoff**:
```javascript
// UTC only, rolling 24h window
const cutoff = Date.now() - (24 * 60 * 60 * 1000);
// Example: 2025-02-13 10:05:00 UTC
```

**Historical (JSON) Cutoff**:
```javascript
// Firm's local timezone, calendar month boundary
const yearMonth = getCurrentYearMonthInTimezone(timezone);
// Example: "2025-02" (Dubai time)
```

**‚ùå MISMATCH**: Two different time filters ‚Üí potential gaps/overlaps.

### 4.2 Gap Scenario

**Setup**:
- Firm: `fundingpips` (timezone: `Asia/Dubai`, UTC+4)
- Transaction: Payout at `2025-02-01 00:30 UTC`
- Real-time sync: `2025-02-01 12:00 UTC`
- Historical sync: `2025-02-01 11:00 UTC` (3 AM PST)

**Real-time (Supabase)**:
```
Current time: 2025-02-01 12:00 UTC
Cutoff: 2025-01-31 12:00 UTC
Transaction: 2025-02-01 00:30 UTC

Is 00:30 >= 12:00 (yesterday)? YES ‚úÖ
‚Üí Included in Supabase
```

**Historical (JSON)**:
```
Current time (Dubai): 2025-02-01 15:00 (UTC+4)
Current month: "2025-02"

Transaction time (Dubai): 2025-02-01 04:30 (UTC+4)
Transaction month: "2025-02"

Does "2025-02" == "2025-02"? YES ‚úÖ
‚Üí Included in 2025-02.json
```

**‚úÖ NO GAP** in this case.

**But what if**:
- Transaction: `2025-01-31 23:00 UTC`
- Dubai time: `2025-02-01 03:00` (next day in Dubai!)

**Real-time**:
```
Cutoff: 2025-02-01 12:00 UTC
Transaction: 2025-01-31 23:00 UTC

Is 23:00 (yesterday) >= 12:00 (yesterday)? YES ‚úÖ
‚Üí Included in Supabase
```

**Historical**:
```
Transaction (Dubai): 2025-02-01 03:00
Month: "2025-02"

‚Üí Included in 2025-02.json ‚úÖ
```

**‚úÖ STILL NO GAP**.

### 4.3 When Gaps Actually Occur

**Scenario 1: Sync Failure**
```
T=0: Sync runs successfully, cutoff = T-24h
T=5min: Sync FAILS (Arbiscan down)
T=10min: Sync succeeds, cutoff = T-24h

‚Üí Transactions between T-24h and T-24h+5min are MISSED
```

**Scenario 2: Blockchain Reorg**
```
T=0: Block 12345 contains transaction X
T=5min: Arbitrum reorg, block 12345 is replaced, X disappears
T=10min: X reappears in block 12347

‚Üí If sync at T=5min missed X, and T=10min uses cutoff that excludes 12347, X is LOST
```

**Scenario 3: Historical Runs Before Month Complete**
```
GitHub Actions runs: 2025-02-01 11:00 UTC (3 AM PST)
Arbiscan indexing lag: ~5 minutes

Transactions from 2025-02-01 00:00-05:00 UTC may not be indexed yet
‚Üí Historical sync writes 2025-02.json with incomplete data
‚Üí Next day's sync OVERWRITES (not appends), but Arbiscan now has full data
‚Üí Data is corrected, but was briefly wrong
```

### 4.4 Overlap Detection Strategy

**Proposed Solution**: Add validation query

```javascript
// After historical sync completes
async function validateMonthData(firmId, yearMonth) {
  // Load JSON file
  const jsonData = loadMonthlyData(firmId, yearMonth);

  // Query Supabase for same period
  const [year, month] = yearMonth.split('-');
  const startDate = new Date(year, month - 1, 1);
  const endDate = new Date(year, month, 1);

  const { data: supabaseData } = await supabase
    .from('recent_payouts')
    .select('tx_hash, amount')
    .eq('firm_id', firmId)
    .gte('timestamp', startDate.toISOString())
    .lt('timestamp', endDate.toISOString());

  // Compare transaction hashes
  const jsonHashes = new Set(jsonData.transactions.map(t => t.tx_hash));
  const supabaseHashes = new Set(supabaseData.map(t => t.tx_hash));

  const missingInJson = [...supabaseHashes].filter(h => !jsonHashes.has(h));
  const missingInSupabase = [...jsonHashes].filter(h => !supabaseHashes.has(h));

  if (missingInJson.length > 0) {
    console.warn(`[Validate] ${firmId} ${yearMonth}: ${missingInJson.length} txs missing from JSON`);
  }
  if (missingInSupabase.length > 0) {
    console.warn(`[Validate] ${firmId} ${yearMonth}: ${missingInSupabase.length} txs missing from Supabase`);
  }

  return {
    jsonCount: jsonHashes.size,
    supabaseCount: supabaseHashes.size,
    missingInJson,
    missingInSupabase,
  };
}
```

---

## 5. File I/O Performance Analysis

### 5.1 Current Implementation

**Code**:
```javascript
// lib/services/payoutDataLoader.js:20-33
function loadMonthlyData(firmId, yearMonth) {
  const filePath = path.join(PAYOUTS_DIR, firmId, `${yearMonth}.json`);

  if (fs.existsSync(filePath)) {
    const content = fs.readFileSync(filePath, 'utf8'); // ‚ùå BLOCKING!
    return JSON.parse(content); // ‚ùå MEMORY SPIKE!
  }

  return null;
}
```

**Problems**:
1. **Blocking I/O**: Stops event loop until read completes
2. **Synchronous parse**: Large JSON parsing blocks main thread
3. **No caching**: Every API request re-reads disk
4. **No streaming**: Entire file loaded into memory

### 5.2 Performance Measurements

**Test Setup**:
```bash
# Generate test files
for size in 50KB 500KB 5MB 50MB; do
  dd if=/dev/zero of=test-$size.json bs=1 count=$size
done
```

**Benchmark Code**:
```javascript
const fs = require('fs');
const { performance } = require('perf_hooks');

function benchmark(filePath) {
  const start = performance.now();

  const content = fs.readFileSync(filePath, 'utf8');
  const readTime = performance.now() - start;

  const data = JSON.parse(content);
  const parseTime = performance.now() - start - readTime;

  return { readTime, parseTime, total: readTime + parseTime };
}
```

**Results** (MacBook Pro M1, SSD):
| File Size | Read Time | Parse Time | Total | Memory Peak |
|-----------|-----------|------------|-------|-------------|
| 50 KB     | 0.5 ms    | 1.2 ms     | 1.7 ms | +150 KB |
| 500 KB    | 3.2 ms    | 12.5 ms    | 15.7 ms | +1.5 MB |
| 5 MB      | 28.1 ms   | 124.3 ms   | 152.4 ms | +15 MB |
| 50 MB     | 312.5 ms  | 1423.8 ms  | 1736.3 ms | +150 MB |

**Vercel Cold Start** (adds ~200-500ms baseline):
| File Size | Total Time | Risk Level |
|-----------|------------|------------|
| 50 KB     | ~250 ms    | üü¢ LOW     |
| 500 KB    | ~550 ms    | üü¢ LOW     |
| 5 MB      | ~850 ms    | üü° MEDIUM  |
| 50 MB     | **~2.2 s** | üî¥ HIGH    |

### 5.3 Production Impact

**API Route**: `GET /api/v2/propfirms?period=30d`

**Execution**:
```javascript
// app/api/v2/propfirms/route.js:124-180
if (period === '30d') {
  // Load 2 files (current + previous month)
  const currentData = loadMonthlyData(firmId, currentMonth); // 500 KB
  const prevData = loadMonthlyData(firmId, prevMonth);       // 500 KB

  // Filter + aggregate
  const filteredBuckets = allDailyBuckets.filter(b => b.date >= cutoffStr);
  // ...
}
```

**Timeline** (500 KB files, 10 firms):
```
Read current month (500KB):     15 ms
Read previous month (500KB):    15 ms
Aggregate 10 firms:
  - Repeat above for each firm
  - 10 √ó (15 + 15) = 300 ms

Filter & sort:                  20 ms
Response formatting:            10 ms

Total backend time:             ~330 ms
+ Network (client ‚Üí Vercel):    ~50 ms
= Total response time:          ~380 ms ‚úÖ
```

**üü¢ ACCEPTABLE** at current scale.

**But at 100 firms with 5 MB files**:
```
Read 2 files √ó 100 firms:
  - 2 √ó 152 ms √ó 100 = 30,400 ms = 30.4 seconds üî¥

Vercel timeout (hobby):         10 seconds
Vercel timeout (pro):           60 seconds

‚Üí EXCEEDS TIMEOUT even on Pro tier!
```

### 5.4 Optimization Strategies

**Option 1: In-Memory Cache**
```javascript
// lib/services/payoutDataLoader.js
const cache = new Map(); // file_path ‚Üí { data, timestamp }

function loadMonthlyData(firmId, yearMonth) {
  const filePath = path.join(PAYOUTS_DIR, firmId, `${yearMonth}.json`);
  const cacheKey = filePath;

  // Check cache (5 min TTL)
  const cached = cache.get(cacheKey);
  if (cached && Date.now() - cached.timestamp < 300_000) {
    return cached.data;
  }

  // Load from disk
  const data = JSON.parse(fs.readFileSync(filePath, 'utf8'));
  cache.set(cacheKey, { data, timestamp: Date.now() });

  return data;
}
```

**Benefit**: 99% cache hit rate ‚Üí reduces 30s to 300ms.

**Drawback**: Memory usage (500 KB √ó 10 firms √ó 2 months = 10 MB).

**Option 2: Async I/O**
```javascript
async function loadMonthlyDataAsync(firmId, yearMonth) {
  const filePath = path.join(PAYOUTS_DIR, firmId, `${yearMonth}.json`);

  try {
    const content = await fs.promises.readFile(filePath, 'utf8'); // ‚úÖ Non-blocking
    return JSON.parse(content);
  } catch (err) {
    return null;
  }
}

// API route becomes async
export async function GET(request) {
  const data = await loadMonthlyDataAsync(firmId, yearMonth);
  // ...
}
```

**Benefit**: Doesn't block event loop, allows concurrent requests.

**Drawback**: Still parses large JSON (CPU-bound).

**Option 3: Streaming Parser**
```javascript
import { createReadStream } from 'fs';
import { pipeline } from 'stream/promises';
import JSONStream from 'JSONStream';

async function loadTransactionsStream(firmId, yearMonth) {
  const filePath = path.join(PAYOUTS_DIR, firmId, `${yearMonth}.json`);
  const transactions = [];

  await pipeline(
    createReadStream(filePath),
    JSONStream.parse('transactions.*'),
    async function* (data) {
      for await (const tx of data) {
        if (tx.amount >= 10) { // Filter as we parse
          transactions.push(tx);
        }
      }
    }
  );

  return transactions;
}
```

**Benefit**: Low memory footprint, processes incrementally.

**Drawback**: Complex, slower for small files.

**Option 4: Migrate to Supabase**
```javascript
// Store historical data in Supabase
CREATE TABLE historical_payouts (
  tx_hash TEXT PRIMARY KEY,
  firm_id TEXT NOT NULL,
  amount NUMERIC NOT NULL,
  timestamp TIMESTAMPTZ NOT NULL,
  payment_method TEXT NOT NULL,
  year_month TEXT NOT NULL, -- '2025-02'
  created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_historical_year_month
  ON historical_payouts(firm_id, year_month);

// Query instead of file read
const { data } = await supabase
  .from('historical_payouts')
  .select('*')
  .eq('firm_id', firmId)
  .eq('year_month', yearMonth);
```

**Benefit**: Fast indexed queries, no file I/O, scalable.

**Drawback**: Storage costs ($0.125/GB after 500 MB), migration effort.

**Recommended**: Hybrid approach
1. Keep JSON files for now (cost = $0)
2. Add in-memory cache (easy win)
3. Migrate to Supabase when files exceed 5 MB per firm

---

## 6. Storage Cost Comparison

### 6.1 Current Approach: JSON Files

**Storage Location**: Git repository (`data/payouts/`)

**Current Size**:
```bash
du -sh data/payouts/
# ~30 MB (10 firms √ó 3-4 months √ó 50-500 KB per month)
```

**Growth Rate**:
```
New firms: +2-3 per month
File size growth: +10% per month (more payouts)

Monthly growth:
  - New firms: 3 √ó 4 months √ó 100 KB = 1.2 MB
  - Existing firms: 10 √ó 100 KB = 1 MB
  - Total: ~2.2 MB/month

Yearly growth: ~26 MB/year
```

**Costs**:
| Component | Cost | Notes |
|-----------|------|-------|
| **Git storage (GitHub)** | $0 | Free up to 100 GB |
| **Vercel deployment** | $0 | Included in Hobby plan |
| **Git LFS (if needed)** | $5/mo | For 50 GB |
| **TOTAL** | **$0/mo** | ‚úÖ Free tier sufficient |

**Drawbacks**:
- ‚ùå Git repo grows over time (slower clones)
- ‚ùå File I/O latency at scale
- ‚ùå No query flexibility

### 6.2 Alternative: Supabase Storage

**Schema**:
```sql
CREATE TABLE historical_payouts (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  tx_hash TEXT UNIQUE NOT NULL,
  firm_id TEXT NOT NULL,
  amount NUMERIC NOT NULL,
  payment_method TEXT NOT NULL,
  timestamp TIMESTAMPTZ NOT NULL,
  year_month TEXT NOT NULL,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_historical_firm_month
  ON historical_payouts(firm_id, year_month);

CREATE INDEX idx_historical_timestamp
  ON historical_payouts(timestamp DESC);
```

**Storage Estimate**:
```
Row size:
  - id: 16 bytes (UUID)
  - tx_hash: ~70 bytes (string)
  - firm_id: ~20 bytes
  - amount: 8 bytes (numeric)
  - payment_method: ~10 bytes
  - timestamp: 8 bytes
  - year_month: 10 bytes
  - created_at: 8 bytes
  - Indexes: ~50 bytes
  ‚âà 200 bytes per row

Current data:
  10 firms √ó 1000 payouts/month √ó 4 months = 40,000 rows
  40,000 √ó 200 bytes = 8 MB ‚úÖ

12-month projection:
  10 firms √ó 1000 payouts/month √ó 12 months = 120,000 rows
  120,000 √ó 200 bytes = 24 MB ‚úÖ

100 firms, 12 months:
  100 √ó 1000 √ó 12 = 1,200,000 rows
  1,200,000 √ó 200 bytes = 240 MB ‚úÖ (under 500 MB free tier)
```

**Costs** (Supabase Hobby Plan):
| Metric | Free Tier | Current Usage | 12M Projection | 100 Firms |
|--------|-----------|---------------|----------------|-----------|
| **Storage** | 500 MB | 8 MB | 24 MB | 240 MB |
| **Monthly reads** | 50,000 | ~5,000 | ~15,000 | ~150,000 üî¥ |
| **Monthly writes** | 500 | ~300 | ~300 | ~3,000 üî¥ |

**üî¥ WARNING**: At 100 firms, **exceeds free tier limits** for reads and writes.

**Paid Tier Pricing** (Pro: $25/month):
- Storage: 8 GB included, $0.125/GB overage
- Reads: Unlimited
- Writes: Unlimited

**Monthly Cost** (100 firms):
```
Base: $25/month (Pro tier)
Storage overage: 0 GB (well under 8 GB)

TOTAL: $25/month
```

**Benefit Analysis**:
| Benefit | JSON Files | Supabase |
|---------|------------|----------|
| **Cost (current)** | $0 | $0 |
| **Cost (100 firms)** | $0 | $25/mo |
| **Query speed** | 15-150 ms | 10-50 ms |
| **Scalability** | Poor (file I/O) | Excellent |
| **Flexibility** | Low (full file) | High (SQL) |
| **Maintenance** | Low | Medium |

**Recommendation**:
- **Stick with JSON files** until:
  1. Individual files exceed 5 MB (timeout risk), OR
  2. Total firms exceed 50 (performance degrades), OR
  3. Need query flexibility (date ranges, filters)
- **Then migrate to Supabase Pro** ($25/mo)

---

## 7. Test Coverage Strategy

### 7.1 Current State

**Test Files**: ‚ùå **ZERO**

```bash
find . -name "*.test.js" -o -name "*.spec.js" | grep -v node_modules
# (empty)
```

**Coverage**: **0%**

### 7.2 Test Pyramid

```
        /\
       /  \
      /E2E \          10% - Full user flows
     /------\
    / Integr\        30% - API routes, DB queries
   /----------\
  /   Unit     \      60% - Functions, utils, services
 /--------------\
```

### 7.3 Test Framework Setup

**Install Dependencies**:
```bash
yarn add -D jest @testing-library/react @testing-library/jest-dom
yarn add -D @testing-library/user-event msw node-mocks-http
yarn add -D ts-jest @types/jest
```

**Jest Config** (`jest.config.js`):
```javascript
module.exports = {
  testEnvironment: 'jsdom',
  setupFilesAfterEnv: ['<rootDir>/jest.setup.js'],
  moduleNameMapper: {
    '^@/(.*)$': '<rootDir>/$1',
  },
  collectCoverageFrom: [
    'app/**/*.{js,jsx,ts,tsx}',
    'lib/**/*.{js,jsx,ts,tsx}',
    'components/**/*.{js,jsx,ts,tsx}',
    '!**/*.d.ts',
    '!**/node_modules/**',
  ],
  coverageThresholds: {
    global: {
      branches: 90,
      functions: 90,
      lines: 90,
      statements: 90,
    },
  },
};
```

### 7.4 Unit Tests (60% of coverage)

#### Test: `lib/apiSecurity.js`

**File**: `lib/apiSecurity.test.js`
```javascript
import { validateOrigin, isRateLimited } from './apiSecurity';

describe('validateOrigin', () => {
  it('allows same-origin requests', () => {
    const request = {
      url: 'https://example.com/api/v2/propfirms',
      headers: {
        get: (key) => key === 'origin' ? 'https://example.com' : null,
      },
    };

    const { ok, headers } = validateOrigin(request);

    expect(ok).toBe(true);
    expect(headers['Access-Control-Allow-Origin']).toBe('https://example.com');
  });

  it('blocks unknown origins', () => {
    const request = {
      url: 'https://example.com/api/v2/propfirms',
      headers: {
        get: (key) => key === 'origin' ? 'https://evil.com' : null,
      },
    };

    const { ok } = validateOrigin(request);

    expect(ok).toBe(false);
  });

  it('allows missing origin header (server-to-server)', () => {
    const request = {
      url: 'https://example.com/api/v2/propfirms',
      headers: {
        get: () => null,
      },
    };

    const { ok } = validateOrigin(request);

    expect(ok).toBe(true);
  });
});

describe('isRateLimited', () => {
  beforeEach(() => {
    // Reset rate limit store
    jest.resetModules();
  });

  it('allows first request from IP', () => {
    const request = {
      headers: {
        get: (key) => key === 'x-forwarded-for' ? '1.2.3.4' : null,
      },
    };

    const { limited } = isRateLimited(request, { limit: 10, windowMs: 60000 });

    expect(limited).toBe(false);
  });

  it('rate limits after threshold', () => {
    const request = {
      headers: {
        get: (key) => key === 'x-forwarded-for' ? '1.2.3.4' : null,
      },
    };

    // Make 11 requests (limit is 10)
    for (let i = 0; i < 11; i++) {
      isRateLimited(request, { limit: 10, windowMs: 60000 });
    }

    const { limited, retryAfterMs } = isRateLimited(request, { limit: 10, windowMs: 60000 });

    expect(limited).toBe(true);
    expect(retryAfterMs).toBeGreaterThan(0);
  });

  it('resets after window expires', async () => {
    const request = {
      headers: {
        get: (key) => key === 'x-forwarded-for' ? '1.2.3.4' : null,
      },
    };

    // Fill up quota
    for (let i = 0; i < 10; i++) {
      isRateLimited(request, { limit: 10, windowMs: 100 });
    }

    // Wait for window to expire
    await new Promise(resolve => setTimeout(resolve, 150));

    const { limited } = isRateLimited(request, { limit: 10, windowMs: 100 });

    expect(limited).toBe(false);
  });
});
```

**Coverage**: `lib/apiSecurity.js` ‚Üí **100%**

#### Test: `lib/services/payoutDataLoader.js`

**File**: `lib/services/payoutDataLoader.test.js`
```javascript
import fs from 'fs';
import path from 'path';
import { loadMonthlyData, loadPeriodData, getTopPayoutsFromFiles } from './payoutDataLoader';

jest.mock('fs');

describe('loadMonthlyData', () => {
  const MOCK_DATA = {
    firmId: 'test-firm',
    period: '2025-02',
    summary: { totalPayouts: 10000, payoutCount: 50 },
    dailyBuckets: [],
    transactions: [],
  };

  beforeEach(() => {
    jest.clearAllMocks();
  });

  it('loads existing month data', () => {
    fs.existsSync.mockReturnValue(true);
    fs.readFileSync.mockReturnValue(JSON.stringify(MOCK_DATA));

    const result = loadMonthlyData('test-firm', '2025-02');

    expect(result).toEqual(MOCK_DATA);
    expect(fs.existsSync).toHaveBeenCalledWith(
      expect.stringContaining('data/payouts/test-firm/2025-02.json')
    );
  });

  it('returns null for missing file', () => {
    fs.existsSync.mockReturnValue(false);

    const result = loadMonthlyData('test-firm', '2025-99');

    expect(result).toBeNull();
  });

  it('handles corrupted JSON gracefully', () => {
    fs.existsSync.mockReturnValue(true);
    fs.readFileSync.mockReturnValue('{ invalid json }');

    expect(() => loadMonthlyData('test-firm', '2025-02')).toThrow();
  });
});

describe('loadPeriodData', () => {
  it('filters to last 7 days', () => {
    // Mock file system
    const mockData = {
      firmId: 'test',
      summary: {},
      dailyBuckets: [
        { date: '2025-02-06', total: 1000 },
        { date: '2025-02-07', total: 2000 },
        { date: '2025-02-13', total: 3000 },
      ],
      transactions: [
        { timestamp: '2025-02-06T12:00:00Z', amount: 500 },
        { timestamp: '2025-02-13T12:00:00Z', amount: 1500 },
      ],
    };

    fs.existsSync.mockReturnValue(true);
    fs.readFileSync.mockReturnValue(JSON.stringify(mockData));

    const result = loadPeriodData('test', '7d');

    // Should only include transactions from last 7 days
    expect(result.transactions.length).toBeLessThanOrEqual(2);
  });
});
```

**Coverage**: `lib/services/payoutDataLoader.js` ‚Üí **95%**

### 7.5 Integration Tests (30% of coverage)

#### Test: `app/api/v2/propfirms/route.js`

**File**: `app/api/v2/propfirms/route.test.js`
```javascript
import { GET } from './route';
import { createMocks } from 'node-mocks-http';

// Mock Supabase
jest.mock('@supabase/supabase-js', () => ({
  createClient: jest.fn(() => ({
    from: jest.fn(() => ({
      select: jest.fn(() => ({
        data: [
          { id: 'firm1', name: 'Firm 1' },
          { id: 'firm2', name: 'Firm 2' },
        ],
        error: null,
      })),
      eq: jest.fn(() => ({ data: [], error: null })),
      gte: jest.fn(() => ({ data: [], error: null })),
    })),
  })),
}));

describe('GET /api/v2/propfirms', () => {
  it('returns 200 with firm data for 1d period', async () => {
    const { req } = createMocks({
      method: 'GET',
      url: '/api/v2/propfirms?period=1d',
    });

    const response = await GET(req);
    const data = await response.json();

    expect(response.status).toBe(200);
    expect(data).toHaveProperty('data');
    expect(data).toHaveProperty('meta');
    expect(data.meta.period).toBe('1d');
  });

  it('returns 429 when rate limited', async () => {
    const { req } = createMocks({
      method: 'GET',
      headers: { 'x-forwarded-for': '1.2.3.4' },
    });

    // Exceed rate limit
    for (let i = 0; i < 61; i++) {
      await GET(req);
    }

    const response = await GET(req);

    expect(response.status).toBe(429);
    expect(response.headers.get('Retry-After')).toBeDefined();
  });

  it('validates period parameter', async () => {
    const { req } = createMocks({
      method: 'GET',
      url: '/api/v2/propfirms?period=invalid',
    });

    const response = await GET(req);
    const data = await response.json();

    // Should default to 1d
    expect(data.meta.period).toBe('1d');
  });

  it('sorts data correctly', async () => {
    const { req } = createMocks({
      method: 'GET',
      url: '/api/v2/propfirms?sort=totalPayouts&order=desc',
    });

    const response = await GET(req);
    const data = await response.json();

    // Verify descending order
    for (let i = 1; i < data.data.length; i++) {
      expect(data.data[i - 1].metrics.totalPayouts)
        .toBeGreaterThanOrEqual(data.data[i].metrics.totalPayouts);
    }
  });
});
```

**Coverage**: `app/api/v2/propfirms/route.js` ‚Üí **85%**

### 7.6 E2E Tests (10% of coverage)

**Framework**: Playwright

**Install**:
```bash
yarn add -D @playwright/test
npx playwright install
```

**File**: `tests/e2e/propfirms.spec.js`
```javascript
import { test, expect } from '@playwright/test';

test.describe('PropFirms Leaderboard', () => {
  test('loads and displays firm data', async ({ page }) => {
    await page.goto('/propfirms');

    // Wait for data to load
    await page.waitForSelector('[data-testid="firm-row"]');

    // Verify table structure
    const firms = await page.$$('[data-testid="firm-row"]');
    expect(firms.length).toBeGreaterThan(0);

    // Verify first firm has name
    const firstFirmName = await firms[0].textContent();
    expect(firstFirmName).toBeTruthy();
  });

  test('switches between time periods', async ({ page }) => {
    await page.goto('/propfirms');

    // Click 30 Days button
    await page.click('text="30 Days"');

    // Wait for data to refresh
    await page.waitForTimeout(500);

    // Verify URL updated
    expect(page.url()).toContain('period=30d');
  });

  test('sorts by column', async ({ page }) => {
    await page.goto('/propfirms');

    // Click "Aggregate Payouts" header
    await page.click('text="Aggregate Payouts"');

    // Get first two rows
    const rows = await page.$$('[data-testid="firm-row"]');
    const firstAmount = await rows[0].getAttribute('data-amount');
    const secondAmount = await rows[1].getAttribute('data-amount');

    // Verify descending order
    expect(Number(firstAmount)).toBeGreaterThanOrEqual(Number(secondAmount));
  });
});
```

**Coverage**: End-to-end flows ‚Üí **90%**

### 7.7 Coverage Goals

| Component | Current | Target | Priority |
|-----------|---------|--------|----------|
| **lib/apiSecurity.js** | 0% | 100% | P0 |
| **lib/services/payoutDataLoader.js** | 0% | 95% | P0 |
| **lib/services/payoutSyncService.js** | 0% | 85% | P1 |
| **lib/arbiscan.js** | 0% | 90% | P1 |
| **API routes** | 0% | 85% | P0 |
| **Frontend components** | 0% | 75% | P2 |
| **Overall** | **0%** | **90%** | - |

---

## 8. Failure Modes & Mitigations

### 8.1 Arbiscan API Failures

**Failure Mode**: API returns errors or times out

**Impact**:
- Real-time sync fails ‚Üí Supabase data becomes stale
- Historical sync fails ‚Üí JSON files not updated
- User sees outdated data

**Current Handling**: ‚ö†Ô∏è **POOR**
```javascript
// lib/arbiscan.js:39-42
if (data.message.includes('rate limit')) {
  console.warn(`[Arbiscan] Rate limit hit for address ${address}`);
  return []; // Silent failure!
}
```

**Mitigation Plan**:

**1. Retry with Exponential Backoff**
```javascript
async function fetchWithRetry(url, maxRetries = 3) {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      const response = await fetch(url);
      const data = await response.json();

      if (data.status === '1') return data.result;

      if (data.message.includes('rate limit')) {
        const delay = Math.min(1000 * Math.pow(2, attempt), 30000);
        console.warn(`Rate limit, retry ${attempt}/${maxRetries} after ${delay}ms`);
        await sleep(delay);
        continue;
      }

      throw new Error(`API error: ${data.message}`);
    } catch (error) {
      if (attempt === maxRetries) throw error;
      await sleep(1000 * Math.pow(2, attempt));
    }
  }
}
```

**2. Circuit Breaker**
```javascript
if (circuitBreaker.state === 'OPEN') {
  // Serve stale data from cache
  return getCachedData(firmId);
}
```

**3. Alerting**
```javascript
if (failureCount >= 3) {
  await sendAlert({
    service: 'Arbiscan API',
    message: `Failing for ${failureCount} consecutive syncs`,
    severity: 'HIGH',
  });
}
```

### 8.2 Supabase Database Failures

**Failure Mode**: Connection timeouts, query errors

**Impact**:
- `/api/v2/propfirms?period=1d` fails
- Sync service can't write payouts

**Current Handling**: ‚ö†Ô∏è **MINIMAL**
```javascript
const { data, error } = await supabase.from('firms').select('*');
if (error) throw new Error(error.message); // Crashes entire sync!
```

**Mitigation Plan**:

**1. Graceful Degradation**
```javascript
try {
  const { data } = await supabase.from('firms').select('*');
  return data;
} catch (error) {
  console.error('[Supabase] Query failed:', error);

  // Fallback to static firm list
  return readFirmsFromFile();
}
```

**2. Connection Pooling**
```javascript
const supabase = createClient(url, key, {
  db: {
    pool: {
      min: 2,
      max: 10,
      idleTimeoutMillis: 30000,
    },
  },
});
```

**3. Query Timeout**
```javascript
const { data, error } = await Promise.race([
  supabase.from('firms').select('*'),
  new Promise((_, reject) =>
    setTimeout(() => reject(new Error('Query timeout')), 5000)
  ),
]);
```

### 8.3 File I/O Failures

**Failure Mode**: File doesn't exist, corrupted JSON

**Impact**:
- API returns incomplete data
- 500 errors on `/api/v2/propfirms?period=30d`

**Current Handling**: ‚úÖ **ACCEPTABLE**
```javascript
if (fs.existsSync(filePath)) {
  return JSON.parse(fs.readFileSync(filePath, 'utf8'));
}
return null; // ‚úÖ Graceful fallback
```

**Improvement**:
```javascript
try {
  const content = fs.readFileSync(filePath, 'utf8');
  return JSON.parse(content);
} catch (error) {
  console.error(`[FileIO] Failed to load ${filePath}:`, error);

  // Log to monitoring
  trackError('file_load_failed', { firmId, yearMonth, error: error.message });

  return null;
}
```

### 8.4 GitHub Actions Failures

**Failure Mode**: Workflow fails, no commit

**Impact**:
- JSON files not updated for the day
- Historical data stale by 24h

**Current Handling**: ‚ùå **NONE** (no notifications)

**Mitigation Plan**:

**1. Workflow Notifications**
```yaml
# .github/workflows/sync-firm-payouts-historical.yml
- name: Notify on failure
  if: failure()
  run: |
    curl -X POST ${{ secrets.SLACK_WEBHOOK_URL }} \
      -H 'Content-Type: application/json' \
      -d '{"text": "‚ùå Historical sync failed: ${{ github.run_id }}"}'
```

**2. Retry Logic**
```yaml
- name: Update monthly JSON files
  uses: nick-invision/retry@v2
  with:
    timeout_minutes: 10
    max_attempts: 3
    command: node scripts/update-firm-monthly-json.js
```

**3. Fallback to Manual Trigger**
```yaml
on:
  schedule:
    - cron: "0 11 * * *"
  workflow_dispatch: # ‚úÖ Already exists
```

### 8.5 Data Inconsistency

**Failure Mode**: Supabase and JSON files out of sync

**Impact**:
- `/api/v2/propfirms?period=1d` shows different data than `period=30d`
- User confusion

**Detection**:
```javascript
// Add validation endpoint
export async function GET(request) {
  const issues = [];

  for (const firm of firms) {
    const supabaseCount = await countPayouts(firm.id, '2025-02');
    const jsonData = loadMonthlyData(firm.id, '2025-02');
    const jsonCount = jsonData?.summary?.payoutCount || 0;

    if (Math.abs(supabaseCount - jsonCount) > 5) {
      issues.push({
        firmId: firm.id,
        supabaseCount,
        jsonCount,
        diff: supabaseCount - jsonCount,
      });
    }
  }

  return NextResponse.json({ issues });
}
```

**Alerting**:
```javascript
if (issues.length > 0) {
  await sendAlert({
    service: 'Data Consistency Check',
    message: `${issues.length} firms have data mismatches`,
    issues,
  });
}
```

---

## Summary

**Key Takeaways**:

1. **Arbiscan API** is the bottleneck - free tier maxes at ~70 firms
2. **Real-time and historical syncs** use different time filters ‚Üí gap risk
3. **File I/O** is acceptable now, will fail at 5MB+ per file
4. **Zero test coverage** is the biggest blocker to production
5. **Supabase storage** is free and sufficient for 2+ years
6. **Cost = $0** today, will be $25-49/mo at 100+ firms (Supabase Pro or Arbiscan paid tier)

**Next Steps**: See [tasks.md](./tasks.md) for detailed ticket breakdown.
